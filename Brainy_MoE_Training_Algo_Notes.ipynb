{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d480ce13",
      "metadata": {
        "id": "d480ce13"
      },
      "source": [
        "# MoE Brainy - Training\n",
        "In this notebook we're going to look at how MoE Brainy is trained.\n",
        "\n",
        "Specifically, we'll examine what happens during a single representative `epoch`, in this case `epoch = 3`.\n",
        "\n",
        "We won't be using actual image information though -- we'll be examining the training code that is run ***after*** the gating network has already determined which experts should be used for each image in the mini batch.\n",
        "\n",
        "Also, we will use mock Experts that generate pre-calculated outputs.\n",
        "\n",
        "Finally, in this example, our mini batch is assumed to contain only 4 images. Normally a mini batch contains 64 images, however by using 4 we will be able to more easily examine the intermediate outputs created by the training code.\n",
        "\n",
        "Much of the training code focuses on vectorizing our data so that we can maximize the parallelism of GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67617bce-0067-48a0-8901-edaf99861642",
      "metadata": {
        "id": "67617bce-0067-48a0-8901-edaf99861642"
      },
      "source": [
        "## Use Bash Instead of PowerShell in Jupyter Lab on Windows\n",
        "Note: On Windows, you get can get a bash terminal working inside Jupyter Lab.\n",
        "\n",
        "1. Make sure you have a full Git installed (e.g.: that includes `winpty` and `bash`)\n",
        "2. Create a file (if it does not already exist) called `jupyter_notebook_config.py` in the `C:\\Users\\{username}\\.jupyter` folder\n",
        "3. Add the code below into this file:\n",
        "\n",
        "```python\n",
        "c = get_config()\n",
        "c.ServerApp.terminado_settings = {\n",
        "    \"shell_command\": [\"C:/Program Files/Git/usr/bin/winpty.exe\", \"C:/Program Files/Git/usr/bin/bash.exe\", \"--login\", \"-i\"]\n",
        "}\n",
        "```\n",
        "\n",
        "4. Double the check that the files referenced above exist, but these are their standard locations.\n",
        "5. If currently in Jupyter Lab, go to `File > Shutdown`\n",
        "6. Restart Jupyter Lab\n",
        "\n",
        "Now when you open a Terminal tab, you will be using bash instead of PowerShell\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "3VK3Daz93dCo"
      },
      "id": "3VK3Daz93dCo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "217f3520",
      "metadata": {
        "id": "217f3520"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "MINI_BATCH_SIZE = 4\n",
        "NUM_EXPERTS = 6\n",
        "TOP_K = 3\n",
        "\n",
        "torch.set_printoptions(\n",
        "    precision=2,       # Number of decimal places\n",
        "    threshold=1000,    # Max number of elements to show\n",
        "    edgeitems=3,       # Number of elements to show at start/end of each dimension\n",
        "    linewidth=120,     # Max width of each line\n",
        "    sci_mode=False     # Use scientific notation\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "162b68d6",
      "metadata": {
        "id": "162b68d6"
      },
      "source": [
        "To simplify the notebook, we're going to create a mock Expert and code it to return a mock set of unactivated predictions (logits) -- one prediction for each digit for each image.\n",
        "\n",
        "We will then instantiate 6 such fake experts for use later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TnKTrcw_uH-C",
      "metadata": {
        "id": "TnKTrcw_uH-C"
      },
      "outputs": [],
      "source": [
        "class FakeExpert:\n",
        "  def __init__(self, expert_num):\n",
        "    self.expert_num = expert_num\n",
        "\n",
        "  def forward(self, mini_batch_size):\n",
        "    return 10 * torch.rand(mini_batch_size, 10)\n",
        "\n",
        "  def __call__(self, *args, **kwargs):\n",
        "    return self.forward(args[0])\n",
        "\n",
        "experts = [FakeExpert(e) for e in range(NUM_EXPERTS)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cc8fd52",
      "metadata": {
        "id": "0cc8fd52"
      },
      "source": [
        "Now, into the main algorithm.\n",
        "\n",
        "The line of code below executes at the start of each epoch. This helps implement the `gradual detachment` idea described in the `README.MD`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b39e32c",
      "metadata": {
        "id": "7b39e32c"
      },
      "outputs": [],
      "source": [
        "epoch = 3\n",
        "current_top_k = max(TOP_K, NUM_EXPERTS - epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a630496",
      "metadata": {
        "id": "6a630496"
      },
      "source": [
        "The `gating_outputs` tensor contains the gating network's score for 6 experts on each of the 4 images in the mini batch.\n",
        "\n",
        "- dim=0: This is the index dimension for the images\n",
        "- dim=1: This is the index dimension for the the experts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b801436",
      "metadata": {
        "id": "5b801436"
      },
      "outputs": [],
      "source": [
        "gating_outputs = torch.tensor([\n",
        "# expert:   0     1     2     3     4     5\n",
        "          [0.30, 0.20, 0.10, 0.25, 0.10, 0.05],  # image 0\n",
        "          [0.05, 0.45, 0.30, 0.05, 0.05, 0.10],  # image 1\n",
        "          [0.10, 0.10, 0.05, 0.50, 0.15, 0.10],  # image 2\n",
        "          [0.25, 0.05, 0.20, 0.10, 0.10, 0.30]   # image 3\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZKJwZPOhm6md",
      "metadata": {
        "id": "ZKJwZPOhm6md"
      },
      "source": [
        "The code below creates a tensor `top_k_indices` that gathers the expert indices (dimension 1 indices) of the top 3 results for each image. So\n",
        "\n",
        "- dim=0: image indices\n",
        "- dim=1: the top `current_top_k` expert indices for the image\n",
        "  - note: each image may have a different set of `current_top_k` experts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ec0c351",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ec0c351",
        "outputId": "d5f69806-a7e1-47b6-88e9-39c546e36d46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 3, 1],\n",
            "        [1, 2, 5],\n",
            "        [3, 4, 0],\n",
            "        [5, 0, 2]])\n"
          ]
        }
      ],
      "source": [
        "_, top_k_indices = torch.topk(input=gating_outputs, k=current_top_k, dim=1)\n",
        "\n",
        "print(top_k_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd03fcc9",
      "metadata": {
        "id": "dd03fcc9"
      },
      "source": [
        "We then create the `mask` tensor which will be used to zero-out any gating results for non-top-k experts. This is important since we are going to normalize the gating network's results so it provides a probability distribution over only the top-k experts.\n",
        "\n",
        "So, step 1 is to create a mask tensor that contains only zeros. This initial zero'd-out mask has the following shape:\n",
        "\n",
        "- dim=0: image indices\n",
        "- dim=1: expert mask indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3G1BYggOnW4n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G1BYggOnW4n",
        "outputId": "2da82668-583c-48e8-aa2b-8fb3fba3a983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mask:\n",
            "tensor([[0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "mask = torch.zeros_like(gating_outputs)\n",
        "\n",
        "print(\"mask:\")\n",
        "print(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f91755f",
      "metadata": {
        "id": "9f91755f"
      },
      "source": [
        "Recall that this tensor is indexed as follows: `[image_index, expert_index]`.\n",
        "\n",
        "What we want to do is set `mask[i,e]` to `1` if expert `e` is one of the top-k experts for image `i`.\n",
        "\n",
        "The `scatter_` method does the equivalent of the following code:\n",
        "\n",
        "```python\n",
        "for i in range(top_k_indices.size(0)): # over all indices in dim=0\n",
        "  for e in top_k_indices[i]: # over a subset of indices in dim=1\n",
        "    mask[i, e] = 1\n",
        "```\n",
        "\n",
        "Note that we set `dim=1` in the call to `scatter_` since `dim=1` specifies the 'independent' coordinate, which in this case is `e`.\n",
        "\n",
        "\\\n",
        "## Side notes on tensor shapes as index sets (feel free to ignore)\n",
        "\n",
        "As an example of what the `dim` argument in `scatter_()` means, consider the example of a 4d tensor `t` and assume we set `dim=1` in the call to `scatter_()`. This means that we range over all indexes in all dimensions of the `target` less dimension `1`. Specifically, we'd range over all indexes\n",
        "\n",
        "$\\quad\\quad d_0 = [0,..,t.size(0) - 1] \\times d_2 = [0,..,t.size(2) - 1] \\times d_3 = [0,..,t.size(3) - 1]$\n",
        "\n",
        "However, we only range over a *subset* of indexes in $d_1$ (dimension 1), where\n",
        "\n",
        "$\\quad\\quad d_1 = [0,..,t.size(1) - 1]$.\n",
        "\n",
        "Let $d'_1$ be this *subset* of indexes in $d_1$, and let $\\overline{d'_1}$ be its complement, then the `mask` is the tensor:\n",
        "\n",
        "$\\quad\\quad mask[i_0,i_1,i_2,i_3] = \\begin{cases}\n",
        "      1 & (i_0,i_1,i_2,i_3) \\in d_0 \\times d'_1 \\times d_2 \\times d_3 \\\\      \n",
        "      0 & (i_0,i_1,i_2,i_3) \\in d_0 \\times \\overline{d'_1} \\times d_2 \\times d_3\n",
        "   \\end{cases}\n",
        "$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Equivalent of scatter_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YVzVPhc7ncNz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVzVPhc7ncNz",
        "outputId": "b48858eb-3879-48f1-e218-8ef415343e9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "top_k_indices:\n",
            "tensor([[0, 3, 1],\n",
            "        [1, 2, 5],\n",
            "        [3, 4, 0],\n",
            "        [5, 0, 2]])\n",
            "\n",
            "mask:\n",
            "tensor([[1., 1., 0., 1., 0., 0.],\n",
            "        [0., 1., 1., 0., 0., 1.],\n",
            "        [1., 0., 0., 1., 1., 0.],\n",
            "        [1., 0., 1., 0., 0., 1.]])\n"
          ]
        }
      ],
      "source": [
        "for i in range(top_k_indices.size(0)): # over all indices in dim=0\n",
        "  for e in top_k_indices[i]: # over a subset of indices in dim=1\n",
        "    mask[i, e] = 1\n",
        "\n",
        "print(f\"top_k_indices:\\n{top_k_indices}\\n\")\n",
        "print(f\"mask:\\n{mask}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b03b999",
      "metadata": {
        "id": "7b03b999"
      },
      "source": [
        "We then do an element-wise multiplication of the `gating_outputs` by the `mask` to zero-out the non-expert scores.\n",
        "\n",
        "An element-wise mult (when the tensor shapes are the same) is equvalent to\n",
        "\n",
        "```python\n",
        "for i in range(mask.size(0)):\n",
        "  for j in range(mask.size(1)):\n",
        "    gating_outputs_mask[i,j] = gating_outputs[i,j] * mask[i,j]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oerONuzrnoaz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oerONuzrnoaz",
        "outputId": "0f8c053f-4ddf-4388-bbe4-21a26a63bce6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gating_outputs\n",
            "tensor([[0.30, 0.20, 0.10, 0.25, 0.10, 0.05],\n",
            "        [0.05, 0.45, 0.30, 0.05, 0.05, 0.10],\n",
            "        [0.10, 0.10, 0.05, 0.50, 0.15, 0.10],\n",
            "        [0.25, 0.05, 0.20, 0.10, 0.10, 0.30]])\n",
            "\n",
            "gating_outputs_mask\n",
            "tensor([[0.30, 0.20, 0.00, 0.25, 0.00, 0.00],\n",
            "        [0.00, 0.45, 0.30, 0.00, 0.00, 0.10],\n",
            "        [0.10, 0.00, 0.00, 0.50, 0.15, 0.00],\n",
            "        [0.25, 0.00, 0.20, 0.00, 0.00, 0.30]])\n"
          ]
        }
      ],
      "source": [
        "print(f\"gating_outputs\\n{gating_outputs}\")\n",
        "print()\n",
        "\n",
        "gating_outputs_mask = gating_outputs * mask\n",
        "print(f\"gating_outputs_mask\\n{gating_outputs_mask}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "088c180e",
      "metadata": {
        "id": "088c180e"
      },
      "source": [
        "Next, we sum the results for each image across all top k experts. This will be used as the denominator for the results for each image in order to normalize the results.\n",
        "\n",
        "\\\n",
        "**gating_outputs_sum = gating_outputs_mask.sum(dim=1, keepdim=True)**\n",
        "\n",
        "\n",
        "`gating_outputs_mask` indexes are `[image_index, expert_index]`. Note that we set `dim=1` since the values we want to sum are the expert scores which are indexed in `dim=1`.\n",
        "\n",
        "We use the `keepdim=True` option to ensure that the dimension `gating_outputs_sum` is the same as `gating_outputs_mask` -- specifically, that it stays 2D.\n",
        "\n",
        "However, while the dimension of `gating_outputs_sum` is still the same (2D) as `gating_outputs_mask`-- the shape has changed, since we have summed the values in `dim=1` which reduces the size of `dim=1` from `6` (the original 6 expert values) to `1` (the sum of the 6 expert values).\n",
        "\n",
        "E.g.:\n",
        "- `gating_outputs_mask` has shape `[image_index, expert_index]`\n",
        "- `gating_outputs_sum`  has shape `[image_index, 1]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o7o8Ns90npa7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7o8Ns90npa7",
        "outputId": "1874db06-438d-42a4-f4fb-37f1add0f757"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.75],\n",
            "        [0.85],\n",
            "        [0.75],\n",
            "        [0.75]])\n",
            "\n",
            "shape of gating_outputs:      torch.Size([4, 6])\n",
            "shape of gating_outputs_sum:  torch.Size([4, 1])\n"
          ]
        }
      ],
      "source": [
        "gating_outputs_sum = gating_outputs_mask.sum(dim=1, keepdim=True)\n",
        "\n",
        "print(gating_outputs_sum)\n",
        "print()\n",
        "print(f\"shape of gating_outputs:      {gating_outputs.size()}\")\n",
        "print(f\"shape of gating_outputs_sum:  {gating_outputs_sum.size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63e98eb7",
      "metadata": {
        "id": "63e98eb7"
      },
      "source": [
        "If we had set `keepdim=False`, then the `sum()` function would have turned this is into a 1D tensor of sums:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fb62e63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fb62e63",
        "outputId": "000dea4f-804b-4e98-a7fe-18f7b8ef3318"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.75, 0.85, 0.75, 0.75])\n",
            "\n",
            "shape of gating_outputs_sum when keepdim=False:  torch.Size([4])\n"
          ]
        }
      ],
      "source": [
        "print(gating_outputs_mask.sum(dim=1, keepdim=False))\n",
        "\n",
        "print()\n",
        "print(f\"shape of gating_outputs_sum when keepdim=False:  {gating_outputs_mask.sum(dim=1, keepdim=False).size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f263231d",
      "metadata": {
        "id": "f263231d"
      },
      "source": [
        "We now do a 'broadcasted' division. Its called 'broadcasted' because the `gating_outputs_sum` size in `dim=1` is only 1, and so it needs to be repeated 6 times to match the shape of `gating_outputs_mask` in `dim=1`, so that element-wise division can work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RIf_y6tvvakh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIf_y6tvvakh",
        "outputId": "4b69e5e9-2d5e-4120-87ac-176b5b120f28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of gating_outputs_mask:  torch.Size([4, 6])\n",
            "\n",
            "shape of gating_outputs_sum:  torch.Size([4, 1])\n",
            "\n",
            "gating_outputs_k = gating_outputs_mask / gating_outputs_sum:\n",
            "tensor([[0.40, 0.27, 0.00, 0.33, 0.00, 0.00],\n",
            "        [0.00, 0.53, 0.35, 0.00, 0.00, 0.12],\n",
            "        [0.13, 0.00, 0.00, 0.67, 0.20, 0.00],\n",
            "        [0.33, 0.00, 0.27, 0.00, 0.00, 0.40]])\n",
            "\n",
            "shap of gating_outputs_k:  torch.Size([4, 6])\n"
          ]
        }
      ],
      "source": [
        "print(f\"shape of gating_outputs_mask:  {gating_outputs_mask.size()}\")\n",
        "gating_outputs_k = gating_outputs_mask / gating_outputs_sum\n",
        "\n",
        "print()\n",
        "print(f\"shape of gating_outputs_sum:  {gating_outputs_sum.size()}\")\n",
        "gating_outputs_k = gating_outputs_mask / gating_outputs_sum\n",
        "\n",
        "print()\n",
        "print(f\"gating_outputs_k = gating_outputs_mask / gating_outputs_sum:\\n{gating_outputs_k}\")\n",
        "\n",
        "print()\n",
        "print(f\"shap of gating_outputs_k:  {gating_outputs_k.size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2168d654",
      "metadata": {
        "id": "2168d654"
      },
      "source": [
        "Now we have a new probability distribution over just the top-k experts.\n",
        "\n",
        "**e_outputs**\\\n",
        "`e_outputs` is a tensor used to hold all 10 outputs from each expert for each image\n",
        "\n",
        "The shape of `e_outputs` is `[mini_batch_size, num_experts, 10] = [4, 6, 10]` in our case.\n",
        "\n",
        "So, what this tensor will hold are the digit predictions from each top-k expert for each image.\n",
        "\n",
        "The first step is to create a tensor with `0`'s as the digit prediction of each expert for each image.\n",
        "\n",
        "We will later update this with the digit predictions from each top-k expert for each image, and leave the predictions from the non-top-k experts as `0` for each digit for each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aj732tIuCRf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aj732tIuCRf",
        "outputId": "f2d59c70-71f4-4953-dbba-fccc12442eee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e_outputs:\n",
            "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
          ]
        }
      ],
      "source": [
        "e_outputs = torch.zeros(MINI_BATCH_SIZE, NUM_EXPERTS, 10)\n",
        "\n",
        "print(\"e_outputs:\")\n",
        "print(e_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28493a5a",
      "metadata": {
        "id": "28493a5a"
      },
      "source": [
        "**The Loop**\\\n",
        "The loop we will look at next will -- for each expert -- figure out the set of images that it is in the top-k for, and then add its digit predictions to `e_outputs`.\n",
        "\n",
        "However, before we get into the loop we're going to look some important code that will help us do this while also taking advantage of the paralellism of GPUs.\n",
        "\n",
        "This code allows us to determine which images each expert e is in the top-k for.\n",
        "\n",
        "**The code**\n",
        "\n",
        "`mask_values_for_e = mask[:, e]`\n",
        "\n",
        "`indexes_of_images_needing_expert_e = torch.where(mask_values_for_e == 1)[0]`\n",
        "\n",
        "**Finding the indexes of 1s**\\\n",
        "First let's back up and look at a more general example. Recall that our goal here is to find the indexes of 1s for expert e in the mask.\n",
        "\n",
        "In this example we imagine that tensor_2d contains the 1s for expert e. As you will see below, tensor_2d is a 2D tensor. This means that -- in this case -- we d not have scalar indexes, but 2d indexes.\n",
        "\n",
        "Because of this, the code `torch.where(tensor_2d > 0)` returns a tuple of 2 1D tensors. The first tensor contains the `dim=0` indexes of 1s in `tensor_2d`, and the second tensor contains the `dim=1` indexes of 1s in `tensor_2d`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "218667d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "218667d4",
        "outputId": "9b15945d-01d1-4bc5-d204-c777d36726b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor_2d:\n",
            "tensor([[0, 1, 0, 1],\n",
            "        [1, 0, 1, 0],\n",
            "        [0, 0, 1, 1]])\n",
            "\n",
            "tensor_2d: torch.Size([3, 4])\n",
            "\n",
            "tuple of dimension indexes:\n",
            "(tensor([0, 0, 1, 1, 2, 2]), tensor([1, 3, 0, 2, 2, 3]))\n",
            "\n",
            "tensor_2d[0,1] = 1\n",
            "tensor_2d[0,3] = 1\n",
            "tensor_2d[1,0] = 1\n",
            "tensor_2d[1,2] = 1\n",
            "tensor_2d[2,2] = 1\n",
            "tensor_2d[2,3] = 1\n"
          ]
        }
      ],
      "source": [
        "# Create a 2D tensor\n",
        "tensor_2d = torch.tensor([[0, 1, 0, 1],\n",
        "                          [1, 0, 1, 0],\n",
        "                          [0, 0, 1, 1]])\n",
        "print(f\"tensor_2d:\\n{tensor_2d}\")\n",
        "\n",
        "print()\n",
        "print(f\"tensor_2d: {tensor_2d.size()}\")\n",
        "\n",
        "# This returns a tuple of two tensors:\n",
        "# - First tensor: dim=0 (row) indices where value > 0\n",
        "# - Second tensor: dim=1 (column) indices where value > 0\n",
        "print()\n",
        "indices = torch.where(tensor_2d > 0)\n",
        "print(f\"tuple of dimension indexes:\\n{indices}\")\n",
        "\n",
        "print()\n",
        "d0 = indices[0]\n",
        "d1 = indices[1]\n",
        "\n",
        "for i in range(len(d0)):\n",
        "    print(f\"tensor_2d[{d0[i]},{d1[i]}] = {tensor_2d[d0[0], d1[0]].item()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "880a27cc",
      "metadata": {
        "id": "880a27cc"
      },
      "source": [
        "**Back to The Loop**\\\n",
        "With that background on how the code works out of the way, let's look at the loop from the MoE Brainy training algorithm again.\n",
        "\n",
        "To simplify it, we're going to focus on one iteration of the loop when `e == 1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b29d9fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b29d9fe",
        "outputId": "e4f2a098-45c7-48ce-c2d3-67b9fc09ef1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mask:\n",
            "         0   1   2   3   4   5   <- experts\n",
            "tensor([[1., 1., 0., 1., 0., 0.],\n",
            "        [0., 1., 1., 0., 0., 1.],\n",
            "        [1., 0., 0., 1., 1., 0.],\n",
            "        [1., 0., 1., 0., 0., 1.]])\n",
            "\n",
            "mask_values_for_e = mask[:,1]:\n",
            "tensor([1., 1., 0., 0.])\n"
          ]
        }
      ],
      "source": [
        "print(f\"mask:\")\n",
        "print(\"         0   1   2   3   4   5   <- experts\")\n",
        "print(f\"{mask}\")\n",
        "\n",
        "print()\n",
        "mask_values_for_e = mask[:,1]\n",
        "print(f\"mask_values_for_e = mask[:,1]:\")\n",
        "print(f\"{mask_values_for_e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "022b5213",
      "metadata": {
        "id": "022b5213"
      },
      "source": [
        "Note that, unlike `tensor_2d`, the tensor containing the 1s we need is not 2d, but 1d. Specifically, `mask_values_for_e` is a 1d tensor of 0s and 1s where:\n",
        "\n",
        "- `mask_values_for_e[i] == 0` if e is not in the top k for image i\n",
        "- `mask_values_for_e[i] == 1` if e IS in the top k for image i\n",
        "\n",
        "Since `mask_values_for_e` is a 1D tensor, then we only need one coordinate to describe the locations of the 1s.\n",
        "\n",
        "Hence there is only one tensor element in the tuple, which we access via `[0]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e8b82e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e8b82e5",
        "outputId": "390bf964-91c6-403f-a226-1c10b17766c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.where(mask_values_for_e == 1):\n",
            "(tensor([0, 1]),)\n",
            "\n",
            "indexes_of_images_needing_expert_e = torch.where(mask_values_for_e == 1)[0]:\n",
            "tensor([0, 1])\n"
          ]
        }
      ],
      "source": [
        "print(f\"torch.where(mask_values_for_e == 1):\")\n",
        "print(f\"{torch.where(mask_values_for_e == 1)}\")\n",
        "\n",
        "print()\n",
        "print(f\"indexes_of_images_needing_expert_e = torch.where(mask_values_for_e == 1)[0]:\")\n",
        "print(f\"{torch.where(mask_values_for_e == 1)[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4413a5d8",
      "metadata": {
        "id": "4413a5d8"
      },
      "source": [
        "We can now continue on to look at the remainder of this loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2bcda5e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2bcda5e",
        "outputId": "9ecc8181-09b3-45a3-d272-c22c3dfcdfb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loop iteration: expert = 0:\n",
            "\n",
            "indexes_of_images_needing_expert_e=0:  tensor([0, 2, 3])\n",
            "num_images_needing_expert_e=0 is 3\n",
            "\n",
            "outputs:\n",
            "tensor([[8.37, 9.35, 4.30, 0.03, 0.19, 8.01, 9.57, 1.21, 0.44, 6.05],\n",
            "        [8.34, 3.48, 5.42, 6.55, 7.49, 2.29, 9.57, 1.71, 1.03, 1.25],\n",
            "        [7.16, 4.13, 7.49, 0.48, 1.39, 0.28, 0.54, 2.23, 8.38, 3.78]])\n",
            "\n",
            "-------\n",
            "\n",
            "loop iteration: expert = 1:\n",
            "\n",
            "indexes_of_images_needing_expert_e=1:  tensor([0, 1])\n",
            "num_images_needing_expert_e=1 is 2\n",
            "\n",
            "outputs:\n",
            "tensor([[8.14, 6.23, 4.33, 5.05, 6.93, 9.46, 6.21, 9.78, 1.63, 4.27],\n",
            "        [5.55, 0.87, 7.86, 2.20, 3.41, 5.59, 9.28, 8.16, 2.25, 4.19]])\n",
            "\n",
            "-------\n",
            "\n",
            "loop iteration: expert = 2:\n",
            "\n",
            "indexes_of_images_needing_expert_e=2:  tensor([1, 3])\n",
            "num_images_needing_expert_e=2 is 2\n",
            "\n",
            "outputs:\n",
            "tensor([[9.51, 1.44, 9.51, 8.69, 5.83, 3.45, 1.59, 3.82, 3.13, 8.46],\n",
            "        [0.18, 6.39, 3.76, 7.09, 1.49, 9.76, 7.31, 6.03, 8.88, 5.22]])\n",
            "\n",
            "-------\n",
            "\n",
            "loop iteration: expert = 3:\n",
            "\n",
            "indexes_of_images_needing_expert_e=3:  tensor([0, 2])\n",
            "num_images_needing_expert_e=3 is 2\n",
            "\n",
            "outputs:\n",
            "tensor([[0.41, 5.68, 6.55, 8.10, 3.52, 5.90, 5.62, 4.91, 1.65, 3.80],\n",
            "        [6.85, 2.81, 8.68, 3.21, 0.62, 5.37, 4.24, 6.31, 0.55, 0.59]])\n",
            "\n",
            "-------\n",
            "\n",
            "loop iteration: expert = 4:\n",
            "\n",
            "indexes_of_images_needing_expert_e=4:  tensor([2])\n",
            "num_images_needing_expert_e=4 is 1\n",
            "\n",
            "outputs:\n",
            "tensor([[5.45, 6.49, 1.21, 7.97, 4.64, 7.66, 0.34, 0.44, 6.25, 8.39]])\n",
            "\n",
            "-------\n",
            "\n",
            "loop iteration: expert = 5:\n",
            "\n",
            "indexes_of_images_needing_expert_e=5:  tensor([1, 3])\n",
            "num_images_needing_expert_e=5 is 2\n",
            "\n",
            "outputs:\n",
            "tensor([[7.49, 5.93, 9.97, 6.84, 2.65, 5.04, 5.65, 0.95, 5.74, 5.91],\n",
            "        [6.54, 0.08, 6.06, 0.41, 5.83, 3.22, 8.66, 3.37, 2.62, 2.79]])\n",
            "\n",
            "-------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for e in range(NUM_EXPERTS):\n",
        "    print(f\"loop iteration: expert = {e}:\\n\")\n",
        "\n",
        "    mask_values_for_e = mask[:, e]\n",
        "    indexes_of_images_needing_expert_e = torch.where(mask_values_for_e == 1)[0]\n",
        "\n",
        "    print(f\"indexes_of_images_needing_expert_e={e}:  {indexes_of_images_needing_expert_e}\")\n",
        "\n",
        "    if len(indexes_of_images_needing_expert_e) > 0:\n",
        "        num_images_needing_expert_e = len(indexes_of_images_needing_expert_e)\n",
        "        print(f\"num_images_needing_expert_e={e} is {num_images_needing_expert_e}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        print(\"outputs:\")\n",
        "        outputs = experts[e](num_images_needing_expert_e)\n",
        "        print(f\"{outputs}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        e_outputs[indexes_of_images_needing_expert_e, e] = outputs\n",
        "\n",
        "    print(\"-------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8621f27a",
      "metadata": {
        "id": "8621f27a"
      },
      "source": [
        "So let's look at the results. For example, expert 4 we have\n",
        "\n",
        "```python\n",
        "loop iteration: expert = 4:\n",
        "\n",
        "indexes_of_images_needing_expert_e=4:  tensor([2])\n",
        "num_images_needing_expert_e=4 is 1\n",
        "\n",
        "outputs:\n",
        "tensor([[9.07, 8.90, 9.48, 4.50, 2.26, 9.11, 9.32, 2.69, 0.98, 9.20]])\n",
        "```\n",
        "\n",
        "This means that there is only one image from the batch that expert 4 is the expert for. The outputs are its logits for this image. The index of the image its an expert for is 2.\n",
        "\n",
        "From the code we see that\n",
        "\n",
        "```python\n",
        "e_outputs[indexes_of_images_needing_expert_e, e] = outputs\n",
        "```\n",
        "\n",
        "This means we replace `e_outputs[2]` with `[9.07, 8.90, 9.48, 4.50, 2.26, 9.11, 9.32, 2.69, 0.98, 9.20]` (expert 4's output)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94714796",
      "metadata": {
        "id": "94714796",
        "outputId": "9525503b-7bbf-4127-a17f-ef0eb718e0a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[8.37, 9.35, 4.30, 0.03, 0.19, 8.01, 9.57, 1.21, 0.44, 6.05],\n",
            "         [8.14, 6.23, 4.33, 5.05, 6.93, 9.46, 6.21, 9.78, 1.63, 4.27],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [0.41, 5.68, 6.55, 8.10, 3.52, 5.90, 5.62, 4.91, 1.65, 3.80],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]],\n",
            "\n",
            "        [[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [5.55, 0.87, 7.86, 2.20, 3.41, 5.59, 9.28, 8.16, 2.25, 4.19],\n",
            "         [9.51, 1.44, 9.51, 8.69, 5.83, 3.45, 1.59, 3.82, 3.13, 8.46],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [7.49, 5.93, 9.97, 6.84, 2.65, 5.04, 5.65, 0.95, 5.74, 5.91]],\n",
            "\n",
            "        [[8.34, 3.48, 5.42, 6.55, 7.49, 2.29, 9.57, 1.71, 1.03, 1.25],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [6.85, 2.81, 8.68, 3.21, 0.62, 5.37, 4.24, 6.31, 0.55, 0.59],\n",
            "         [5.45, 6.49, 1.21, 7.97, 4.64, 7.66, 0.34, 0.44, 6.25, 8.39],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]],\n",
            "\n",
            "        [[7.16, 4.13, 7.49, 0.48, 1.39, 0.28, 0.54, 2.23, 8.38, 3.78],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [0.18, 6.39, 3.76, 7.09, 1.49, 9.76, 7.31, 6.03, 8.88, 5.22],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [6.54, 0.08, 6.06, 0.41, 5.83, 3.22, 8.66, 3.37, 2.62, 2.79]]])\n"
          ]
        }
      ],
      "source": [
        "print(e_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82d764d6",
      "metadata": {
        "id": "82d764d6",
        "outputId": "e2e37e4c-f6cb-4354-e2d6-6b36baf27627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gating_outputs_k\n",
            "torch.Size([4, 6])\n",
            "tensor([[0.40, 0.27, 0.00, 0.33, 0.00, 0.00],\n",
            "        [0.00, 0.53, 0.35, 0.00, 0.00, 0.12],\n",
            "        [0.13, 0.00, 0.00, 0.67, 0.20, 0.00],\n",
            "        [0.33, 0.00, 0.27, 0.00, 0.00, 0.40]])\n",
            "\n",
            "gate_confidence = gating_outputs_k.unsqueeze(2)\n",
            "torch.Size([4, 6, 1])\n",
            "tensor([[[0.40],\n",
            "         [0.27],\n",
            "         [0.00],\n",
            "         [0.33],\n",
            "         [0.00],\n",
            "         [0.00]],\n",
            "\n",
            "        [[0.00],\n",
            "         [0.53],\n",
            "         [0.35],\n",
            "         [0.00],\n",
            "         [0.00],\n",
            "         [0.12]],\n",
            "\n",
            "        [[0.13],\n",
            "         [0.00],\n",
            "         [0.00],\n",
            "         [0.67],\n",
            "         [0.20],\n",
            "         [0.00]],\n",
            "\n",
            "        [[0.33],\n",
            "         [0.00],\n",
            "         [0.27],\n",
            "         [0.00],\n",
            "         [0.00],\n",
            "         [0.40]]])\n"
          ]
        }
      ],
      "source": [
        "print(f\"gating_outputs_k\\n{gating_outputs_k.size()}\\n{gating_outputs_k}\")\n",
        "\n",
        "gate_confidence = gating_outputs_k.unsqueeze(2)\n",
        "print(f\"\\ngate_confidence = gating_outputs_k.unsqueeze(2)\\n{gate_confidence.size()}\\n{gate_confidence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c13554af",
      "metadata": {
        "id": "c13554af"
      },
      "source": [
        "We then multiply all the top-k experts' logits for each image by gating networks expert-weights.\n",
        "\n",
        "We can see that `gate_confidence`'s non zero rows match exactly with the non-zero rows of `weighted_outputs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e26fb7da",
      "metadata": {
        "id": "e26fb7da",
        "outputId": "6ce638ff-14a2-4163-f649-df76d83a8fc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gate_confidence:\n",
            "tensor([[[0.40],\n",
            "         [0.27],\n",
            "         [0.00],\n",
            "         [0.33],\n",
            "         [0.00],\n",
            "         [0.00]],\n",
            "\n",
            "        [[0.00],\n",
            "         [0.53],\n",
            "         [0.35],\n",
            "         [0.00],\n",
            "         [0.00],\n",
            "         [0.12]],\n",
            "\n",
            "        [[0.13],\n",
            "         [0.00],\n",
            "         [0.00],\n",
            "         [0.67],\n",
            "         [0.20],\n",
            "         [0.00]],\n",
            "\n",
            "        [[0.33],\n",
            "         [0.00],\n",
            "         [0.27],\n",
            "         [0.00],\n",
            "         [0.00],\n",
            "         [0.40]]])\n",
            "\n",
            "            *\n",
            "e_outputs:\n",
            "tensor([[[8.37, 9.35, 4.30, 0.03, 0.19, 8.01, 9.57, 1.21, 0.44, 6.05],\n",
            "         [8.14, 6.23, 4.33, 5.05, 6.93, 9.46, 6.21, 9.78, 1.63, 4.27],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [0.41, 5.68, 6.55, 8.10, 3.52, 5.90, 5.62, 4.91, 1.65, 3.80],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]],\n",
            "\n",
            "        [[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [5.55, 0.87, 7.86, 2.20, 3.41, 5.59, 9.28, 8.16, 2.25, 4.19],\n",
            "         [9.51, 1.44, 9.51, 8.69, 5.83, 3.45, 1.59, 3.82, 3.13, 8.46],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [7.49, 5.93, 9.97, 6.84, 2.65, 5.04, 5.65, 0.95, 5.74, 5.91]],\n",
            "\n",
            "        [[8.34, 3.48, 5.42, 6.55, 7.49, 2.29, 9.57, 1.71, 1.03, 1.25],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [6.85, 2.81, 8.68, 3.21, 0.62, 5.37, 4.24, 6.31, 0.55, 0.59],\n",
            "         [5.45, 6.49, 1.21, 7.97, 4.64, 7.66, 0.34, 0.44, 6.25, 8.39],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]],\n",
            "\n",
            "        [[7.16, 4.13, 7.49, 0.48, 1.39, 0.28, 0.54, 2.23, 8.38, 3.78],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [0.18, 6.39, 3.76, 7.09, 1.49, 9.76, 7.31, 6.03, 8.88, 5.22],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [6.54, 0.08, 6.06, 0.41, 5.83, 3.22, 8.66, 3.37, 2.62, 2.79]]])\n",
            "\n",
            "            =\n",
            "\\weighted_outputs:\n",
            "tensor([[[3.35, 3.74, 1.72, 0.01, 0.08, 3.21, 3.83, 0.48, 0.18, 2.42],\n",
            "         [2.17, 1.66, 1.16, 1.35, 1.85, 2.52, 1.66, 2.61, 0.43, 1.14],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [0.14, 1.89, 2.18, 2.70, 1.17, 1.97, 1.87, 1.64, 0.55, 1.27],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]],\n",
            "\n",
            "        [[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [2.94, 0.46, 4.16, 1.17, 1.80, 2.96, 4.91, 4.32, 1.19, 2.22],\n",
            "         [3.36, 0.51, 3.36, 3.07, 2.06, 1.22, 0.56, 1.35, 1.10, 2.99],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [0.88, 0.70, 1.17, 0.80, 0.31, 0.59, 0.66, 0.11, 0.68, 0.70]],\n",
            "\n",
            "        [[1.11, 0.46, 0.72, 0.87, 1.00, 0.30, 1.28, 0.23, 0.14, 0.17],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [4.56, 1.87, 5.79, 2.14, 0.41, 3.58, 2.83, 4.20, 0.37, 0.39],\n",
            "         [1.09, 1.30, 0.24, 1.59, 0.93, 1.53, 0.07, 0.09, 1.25, 1.68],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]],\n",
            "\n",
            "        [[2.39, 1.38, 2.50, 0.16, 0.46, 0.09, 0.18, 0.74, 2.79, 1.26],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [0.05, 1.70, 1.00, 1.89, 0.40, 2.60, 1.95, 1.61, 2.37, 1.39],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
            "         [2.62, 0.03, 2.42, 0.16, 2.33, 1.29, 3.47, 1.35, 1.05, 1.11]]])\n"
          ]
        }
      ],
      "source": [
        "weighted_outputs = gate_confidence * e_outputs\n",
        "\n",
        "print(f\"gate_confidence:\\n{gate_confidence}\")\n",
        "print(\"\\n            *\")\n",
        "print(f\"e_outputs:\\n{e_outputs}\")\n",
        "print(\"\\n            =\")\n",
        "print(f\"\\weighted_outputs:\\n{weighted_outputs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b180b4e8",
      "metadata": {
        "id": "b180b4e8"
      },
      "source": [
        "Finally, we need to sum along `dim=1` (the experts dimnsion) to get the final weighted digit logits for each image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca9a0f61",
      "metadata": {
        "id": "ca9a0f61",
        "outputId": "51d06493-174e-4d32-eeac-44c5760462d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[3.00, 2.54, 5.47, 3.03, 5.88, 3.21, 6.45, 5.28, 8.33, 8.78],\n",
            "        [4.57, 1.92, 3.18, 5.87, 7.86, 6.32, 8.69, 7.18, 2.49, 4.13],\n",
            "        [3.10, 7.10, 5.62, 1.70, 1.31, 3.36, 1.43, 3.23, 4.09, 4.15],\n",
            "        [3.06, 3.78, 7.14, 3.27, 3.87, 5.48, 4.50, 4.04, 4.18, 4.43]])\n"
          ]
        }
      ],
      "source": [
        "combined_outputs = torch.sum(weighted_outputs, dim=1)\n",
        "\n",
        "print(combined_outputs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}